# åŸºäºLoRAå¾®è°ƒçš„GPT-2åƒåœ¾çŸ­ä¿¡åˆ†ç±»æ¨¡å‹

ä½¿ç”¨LoRAæŠ€æœ¯å¯¹GPT-2æ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œå®ç°çŸ­ä¿¡æ–‡æœ¬çš„äºŒåˆ†ç±»ï¼ˆåƒåœ¾ä¿¡æ¯/æ­£å¸¸ä¿¡æ¯ï¼‰ã€‚ç³»ç»ŸåŒ…å«å®Œæ•´çš„è®­ç»ƒæµç¨‹ã€è¯„ä¼°æ¨¡å—å’ŒåŸºäºchainlitçš„å¯è§†åŒ–äº¤äº’ç•Œé¢ï¼Œé€‚ç”¨äºååƒåœ¾ä¿¡æ¯è¿‡æ»¤åœºæ™¯ã€‚

## é¡¹ç›®ç»“æ„

```
gpt_classification/
â”œâ”€â”€ README.md                       # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ chainlit_gpt_classification.py  # äº¤äº’å¼åˆ†ç±»ç•Œé¢(WebæœåŠ¡ç«¯)
â”œâ”€â”€ gpt_classification_model.py     # æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°æ¨¡å—
â”œâ”€â”€ training_metrics.pdf            # è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–å›¾è¡¨
â””â”€â”€ lora_model/                     # å¾®è°ƒåçš„æ¨¡å‹ç»„ä»¶
    â”œâ”€â”€ adapter_config.json         # LoRAé€‚é…å™¨ç»“æ„é…ç½®
    â”œâ”€â”€ adapter_model.safetensors   # æ¨¡å‹æƒé‡(safetensorsæ ¼å¼)
    â”œâ”€â”€ special_tokens_map.json     # ç‰¹æ®Štokenæ˜ å°„è¡¨
    â”œâ”€â”€ tokenizer_config.json       # åˆ†è¯å™¨é…ç½®å‚æ•°
    â””â”€â”€ ...                         # å…¶ä»–æ¨¡å‹é…ç½®æ–‡ä»¶
â””â”€â”€ dataset/                        # æ•°æ®ç®¡ç†æ¨¡å—
    â”œâ”€â”€ SMSSpamCollection.tsv       # åŸå§‹æ•°æ®é›†
    â”œâ”€â”€ train.csv                   # è®­ç»ƒé›†(70%)
    â”œâ”€â”€ validation.csv              # éªŒè¯é›†(10%)
    â””â”€â”€ test.csv                    # æµ‹è¯•é›†(20%)
```

## é¡¹ç›®è¿è¡Œæµç¨‹

- è®­ç»ƒå¾®è°ƒé˜¶æ®µï¼špython gpt_classification_model.py
- éƒ¨ç½²è¿è¡Œé˜¶æ®µï¼šchainlit run chainlit_gpt_classification.py
- å¤‡æ³¨ï¼šå¯ä»¥ç›´æ¥cloneé¡¹ç›®åä¸€é”®éƒ¨ç½²è¿è¡Œï¼Œé¡¹ç›®å†…å·²æœ‰loraå¾®è°ƒåçš„æ¨¡å‹å‚æ•°

## æ¨¡å‹è®­ç»ƒä¸å¾®è°ƒåŠŸèƒ½è¯¦è§£

### 1. æ•°æ®é¢„å¤„ç†

- è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼ˆåŸå§‹æ•°æ®hamå æ¯”86.6%ï¼‰
- ä¿æŒspam/hamæ ·æœ¬æ¯”ä¾‹1:1
- å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§
- `gpt_classification_model.py`ç›¸å…³ä»£ç 
```python
def create_balanced_dataset(df):
    # ç»Ÿè®¡spamæ ·æœ¬æ•°ï¼ˆå°‘æ•°ç±»ï¼‰
    num_spam = df[df["Label"] == "spam"].shape[0]  
    # å¯¹hamæ ·æœ¬è¿›è¡Œä¸‹é‡‡æ ·
    ham_subset = df[df["Label"] == "ham"].sample(num_spam, random_state=123)
    # åˆå¹¶å¹³è¡¡æ•°æ®é›†
    return pd.concat([ham_subset, df[df["Label"] == "spam"]])
```
  
### 2. åŠ¨æ€æ•°æ®åˆ’åˆ†

- é»˜è®¤æ¯”ä¾‹ï¼šè®­ç»ƒé›†70%ï¼ŒéªŒè¯é›†10%ï¼Œæµ‹è¯•é›†20%
- åŸºäºpandasçš„é«˜æ•ˆæ•°æ®ç®¡ç†
- è¾“å‡ºæ ‡å‡†åŒ–CSVæ ¼å¼ä¾¿äºåç»­å¤„ç†
- `gpt_classification_model.py`ç›¸å…³ä»£ç 
```python
def random_split(df, train_frac=0.7, validation_frac=0.1):
    df = df.sample(frac=1, random_state=123)
    train_end = int(len(df) * train_frac)
    validation_end = train_end + int(len(df) * validation_frac)
    return df[:train_end], df[train_end:validation_end], df[validation_end:]
```

### 3. åˆ†è¯å™¨ä¸æ¨¡å‹å¯¼å…¥

**åˆ†è¯å™¨åˆå§‹åŒ–ä¸é…ç½®**
| å‚æ•°/æ“ä½œ                  | ä½œç”¨                         | å¿…è¦æ€§ | æ³¨æ„äº‹é¡¹                              |
|---------------------------|------------------------------|--------|---------------------------------------|
| `from_pretrained("gpt2")` | åŠ è½½GPT-2é¢„è®­ç»ƒåˆ†è¯å™¨         | âœ…å¿…éœ€  | éœ€ä¸åç»­æ¨¡å‹æ¶æ„ä¸¥æ ¼åŒ¹é…              |
| `pad_token`è®¾ç½®           | å®šä¹‰å¡«å……æ ‡è®°                  | âœ…å…³é”®  | åŸå§‹GPT-2æ— ä¸“ç”¨pad_tokenï¼Œå¿…é¡»æ˜¾å¼è®¾ç½® |
| ä½¿ç”¨`eos_token`           | å°†åºåˆ—ç»“æŸç¬¦å…¼ä½œå¡«å……ç¬¦        | âœ…æ¨èæ–¹æ¡ˆ | éœ€ç¡®ä¿tokenizer.eos_token_idä¸æ¨¡å‹é…ç½®ä¸€è‡´ |
- `gpt_classification_model.py`åˆ†è¯å™¨ç›¸å…³ä»£ç 
```python
tokenizer = AutoTokenizer.from_pretrained("gpt2")  # åˆå§‹åŒ– GPT-2 åˆ†è¯å™¨
tokenizer.pad_token = tokenizer.eos_token          # è®¾ç½® pad token
```
**åº•å±‚æœºåˆ¶**

**1. è¯æ±‡è¡¨å…¼å®¹æ€§**
- é‡‡ç”¨BPE(Byte Pair Encoding)åˆ†è¯æ–¹å¼
- è¯æ±‡è¡¨å¤§å°ï¼š50,257ä¸ªtoken
- å¿…é¡»ä¸æ¨¡å‹embeddingå±‚å®Œå…¨å¯¹é½

**2. å¡«å……å¤„ç†ä¼˜åŒ–**
- è¾“å…¥IDç»“æ„ï¼š`[token_ids] + [EOS] + [PAD,...]`
- æ³¨æ„åŠ›æœºåˆ¶ä¼šè‡ªåŠ¨å¿½ç•¥PADä½ç½®çš„è®¡ç®—

**åˆ†ç±»æ¨¡å‹åŠ è½½**
| å‚æ•°           | å€¼              | ä½œç”¨                              |
|----------------|-----------------|-----------------------------------|
| `"gpt2"`       | æ¨¡å‹æ ‡è¯†         | åŠ è½½125Må‚æ•°çš„GPT-2åŸºç¡€æ¶æ„       |
| `num_labels`   | 2               | æ·»åŠ äºŒåˆ†ç±»è¾“å‡ºå±‚(spam/ham)        |
| `pad_token_id` | `eos_token_id`  | ä½¿æ¨¡å‹å¿½ç•¥å¡«å……ä½ç½®çš„è®¡ç®—           |
- `gpt_classification_model.py`æ¨¡å‹åŠ è½½ç›¸å…³ä»£ç 
```python
# åŠ è½½ GPT-2 æ¨¡å‹ç”¨äºåˆ†ç±»ä»»åŠ¡
model = AutoModelForSequenceClassification.from_pretrained(
    "gpt2", num_labels=2, pad_token_id=tokenizer.eos_token_id
)
```
**æ¶æ„ç‰¹æ€§**

**1. åŸºç¡€æ¶æ„**
- åŸºäºGPT-2 Transformerè§£ç å™¨
- 12ä¸ªéšè—å±‚ï¼Œ768éšè—ç»´åº¦
- 12ä¸ªæ³¨æ„åŠ›å¤´

**2. åˆ†ç±»é€‚é…**
- åœ¨åŸºç¡€æ¨¡å‹é¡¶éƒ¨æ·»åŠ çº¿æ€§åˆ†ç±»å±‚
- è¾“å‡ºç»´åº¦å›ºå®šä¸º2ï¼ˆäºŒåˆ†ç±»ä»»åŠ¡ï¼‰

**3. å‚æ•°åˆå§‹åŒ–**
- ä¸»å¹²ç½‘ç»œä¿æŒé¢„è®­ç»ƒæƒé‡ä¸å˜
- åˆ†ç±»å±‚ä½¿ç”¨éšæœºåˆå§‹åŒ–(Lora)

### 4. LoRAå¾®è°ƒæ¶æ„

- ä»…å¾®è°ƒ0.1%çš„æ¨¡å‹å‚æ•°ï¼ˆçº¦1.1Må¯è®­ç»ƒå‚æ•°ï¼‰
- ä¿æŒGPT-2çš„95%å‚æ•°å†»ç»“
- èŠ‚çœ75%æ˜¾å­˜æ¶ˆè€—ï¼ˆç›¸æ¯”å…¨å‚æ•°å¾®è°ƒï¼‰
- `gpt_classification_model.py`ç›¸å…³Loraä»£ç 
```python
lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,           # åºåˆ—åˆ†ç±»ä»»åŠ¡
    r=8,                                  # ä½ç§©çŸ©é˜µç»´åº¦
    lora_alpha=32,                        # ç¼©æ”¾ç³»æ•°ï¼ˆæ§åˆ¶é€‚é…å™¨å½±å“å¼ºåº¦ï¼‰
    target_modules=["c_attn", "c_proj"],  # ä¿®æ”¹GPT-2çš„æ³¨æ„åŠ›æœºåˆ¶
    lora_dropout=0.1,                     # Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
    bias="none",                          # ä¸è®­ç»ƒåç½®é¡¹
    modules_to_save=["score"]             # å…¨å‚æ•°è®­ç»ƒåˆ†ç±»å±‚
)
```

### 5. è®­ç»ƒå‚æ•°é…ç½®è¯¦è§£
| å‚æ•°åç§°                      | é»˜è®¤å€¼     | ä½œç”¨åŸŸ        | è¯¦ç»†è¯´æ˜                                                                 |
|-------------------------------|------------|--------------|--------------------------------------------------------------------------|
| **output_dir**               | './results' | å…¨å±€é…ç½®      | è®­ç»ƒè¾“å‡ºç›®å½•ï¼ˆåŒ…å«æ¨¡å‹æ£€æŸ¥ç‚¹ã€æ—¥å¿—ç­‰ï¼‰ã€‚LoRAè®­ç»ƒæ—¶å®é™…åªä¿å­˜é€‚é…å™¨å‚æ•°ã€‚   |
| **report_to**                | "none"      | æ—¥å¿—ç³»ç»Ÿ      | ç¦ç”¨ç¬¬ä¸‰æ–¹æŠ¥å‘Šå·¥å…·ï¼ˆå¦‚Weights & Biasesï¼‰ï¼Œé¿å…ä¾èµ–å¤–éƒ¨æœåŠ¡ã€‚             |
| **num_train_epochs**         | 5           | è®­ç»ƒæ§åˆ¶      | æ€»è®­ç»ƒè½®æ¬¡ï¼Œå»ºè®®è®¾ä¸º3-10ä¹‹é—´ï¼Œæ ¹æ®early stoppingåŠ¨æ€è°ƒæ•´ã€‚               |
| **per_device_train_batch_size** | 16        | ç¡¬ä»¶èµ„æº      | å•ä¸ªGPUçš„æ‰¹æ¬¡å¤§å°ï¼ŒT4æ˜¾å¡å»ºè®®8-32ï¼ŒA100å»ºè®®32-128ã€‚                      |
| **per_device_eval_batch_size** | 16        | ç¡¬ä»¶èµ„æº      | éªŒè¯/æµ‹è¯•æ—¶çš„æ‰¹æ¬¡å¤§å°ï¼Œé€šå¸¸ä¸è®­ç»ƒæ‰¹æ¬¡ä¸€è‡´ã€‚                              |
| **learning_rate**            | 1e-4        | ä¼˜åŒ–å™¨        | åˆå§‹å­¦ä¹ ç‡ï¼ŒLoRAè®­ç»ƒå»ºè®®1e-5åˆ°1e-4ï¼Œå…¨å‚æ•°å¾®è°ƒå»ºè®®æ›´å°(5e-6)ã€‚           |
| **eval_strategy**            | "steps"     | è¯„ä¼°ç­–ç•¥      | å¯é€‰"steps"ï¼ˆæŒ‰æ­¥æ•°ï¼‰æˆ–"epoch"ï¼ˆæ¯è½®è¯„ä¼°ï¼‰ï¼ŒåŠ¨æ€è¯„ä¼°æ›´èŠ‚çœæ—¶é—´ã€‚         |
| **eval_steps**               | 100         | è¯„ä¼°ç­–ç•¥      | é…åˆeval_strategy="steps"ï¼Œæ¯è®­ç»ƒ100æ­¥æ‰§è¡Œä¸€æ¬¡éªŒè¯é›†è¯„ä¼°ã€‚               |
| **save_strategy**            | "steps"     | æ¨¡å‹ä¿å­˜      | æ¨¡å‹ä¿å­˜ç­–ç•¥ï¼Œä¸è¯„ä¼°ç­–ç•¥è§£è€¦å¯ç‹¬ç«‹é…ç½®ã€‚                                 |
| **save_steps**               | 100         | æ¨¡å‹ä¿å­˜      | æ¯100æ­¥ä¿å­˜ä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼Œæ³¨æ„ç£ç›˜ç©ºé—´å ç”¨ã€‚                                |
| **logging_dir**              | './logs'    | æ—¥å¿—ç³»ç»Ÿ      | TensorBoardæ—¥å¿—å­˜å‚¨è·¯å¾„ï¼Œéœ€å•ç‹¬å¯åŠ¨ç›‘æ§æœåŠ¡ã€‚                            |
| **logging_strategy**         | "steps"     | æ—¥å¿—ç³»ç»Ÿ      | æ—¥å¿—è®°å½•é¢‘ç‡ï¼Œå»ºè®®ä¸è¯„ä¼°æ­¥è°ƒä¸€è‡´ä¾¿äºåˆ†æã€‚                               |
| **logging_steps**            | 100         | æ—¥å¿—ç³»ç»Ÿ      | æ¯100æ­¥è®°å½•ä¸€æ¬¡è®­ç»ƒæŒ‡æ ‡ï¼ˆloss/lrç­‰ï¼‰ã€‚                                  |
| **load_best_model_at_end**   | True        | æ¨¡å‹é€‰æ‹©      | è®­ç»ƒç»“æŸåè‡ªåŠ¨åŠ è½½éªŒè¯é›†æœ€ä¼˜çš„æ¨¡å‹ç‰ˆæœ¬ã€‚                                 |
| **metric_for_best_model**    | "accuracy"  | æ¨¡å‹é€‰æ‹©      | æœ€ä¼˜æ¨¡å‹åˆ¤å®šæŒ‡æ ‡ï¼Œå¯æ”¹ä¸º"f1"ã€"precision"ç­‰ã€‚                           |
| **greater_is_better**        | True        | æ¨¡å‹é€‰æ‹©      | æŒ‡æ ‡æ–¹å‘ï¼ŒTrueè¡¨ç¤ºæŒ‡æ ‡å€¼è¶Šå¤§è¶Šå¥½ï¼ˆå¦‚accuracyï¼‰ã€‚                        |
| **max_grad_norm**            | 1.0         | ä¼˜åŒ–å™¨        | æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼ŒLoRAè®­ç»ƒå¯é€‚å½“å¢å¤§ï¼ˆå¦‚2.0ï¼‰ã€‚                |
| **lr_scheduler_type**        | "cosine"    | å­¦ä¹ ç‡è°ƒåº¦    | å¯é€‰"linear"ã€"cosine"ã€"constant"ç­‰ï¼Œä½™å¼¦é€€ç«cosineé€šå¸¸æ•ˆæœæœ€ä½³ã€‚      |
| **warmup_ratio**             | 0.1         | å­¦ä¹ ç‡è°ƒåº¦    | 10%çš„è®­ç»ƒæ­¥ç”¨äºå­¦ä¹ ç‡çº¿æ€§é¢„çƒ­ï¼Œé¿å…åˆæœŸä¸ç¨³å®šã€‚                          |
- æ¯100æ­¥è‡ªåŠ¨è®°å½•è®­ç»ƒ/éªŒè¯å‡†ç¡®ç‡
- å®æ—¶è¿½è¸ªæŸå¤±æ›²çº¿
- è‡ªåŠ¨ä¿å­˜æœ€ä¼˜æ¨¡å‹æ£€æŸ¥ç‚¹
- `gpt_classification_model.py`è®­ç»ƒå‚æ•°ç›¸å…³ä»£ç 
```python
training_args = TrainingArguments(
    output_dir='./results',           # è®­ç»ƒè¾“å‡ºç›®å½•(loraå¾®è°ƒæ—¶ä¸éœ€è¦)
    report_to="none",                 # ç¦ç”¨ç¬¬ä¸‰æ–¹æŠ¥å‘Š(å¦‚wandb)
    num_train_epochs=5,               # è®­ç»ƒè½®æ¬¡
    per_device_train_batch_size=16,   # å•è®¾å¤‡è®­ç»ƒæ‰¹æ¬¡å¤§å°
    per_device_eval_batch_size=16,    # å•è®¾å¤‡è¯„ä¼°æ‰¹æ¬¡å¤§å°
    learning_rate=1e-4,               # åˆå§‹å­¦ä¹ ç‡
    eval_strategy="steps",            # æŒ‰æ­¥æ•°è¯„ä¼°
    eval_steps=100,                   # æ¯100æ­¥è¯„ä¼°ä¸€æ¬¡
    save_strategy="steps",            # æŒ‰æ­¥æ•°ä¿å­˜æ¨¡å‹
    save_steps=100,                   # æ¯100æ­¥ä¿å­˜æ£€æŸ¥ç‚¹
    logging_dir='./logs',             # æ—¥å¿—å­˜å‚¨ç›®å½•
    logging_strategy="steps",         # æŒ‰æ­¥æ•°è®°å½•æ—¥å¿—
    logging_steps=100,                # æ¯100æ­¥è®°å½•æ—¥å¿—
    load_best_model_at_end=True,      # è®­ç»ƒç»“æŸæ—¶åŠ è½½æœ€ä¼˜æ¨¡å‹
    metric_for_best_model="accuracy", # æœ€ä¼˜æ¨¡å‹åˆ¤å®šæŒ‡æ ‡
    greater_is_better=True,           # å‡†ç¡®ç‡è¶Šé«˜è¶Šå¥½
    max_grad_norm=1.0,                # æ¢¯åº¦è£å‰ªé˜ˆå€¼
    lr_scheduler_type="cosine",       # ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦
    warmup_ratio=0.1,                 # 10%è®­ç»ƒæ­¥ç”¨äºå­¦ä¹ ç‡é¢„çƒ­
)
```

### 6. è®­ç»ƒå™¨å‚æ•°é…ç½®
| å‚æ•° | ç±»å‹ | å¿…é€‰ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|------|------|
| **model** | `PreTrainedModel` | âœ… | - | éœ€è¦è®­ç»ƒçš„æ¨¡å‹å®ä¾‹ï¼ˆLoRA å¾®è°ƒæ—¶ä¸º `PeftModel` å°è£…åçš„æ¨¡å‹ï¼‰ |
| **args** | `TrainingArguments` | âœ… | - | è®­ç»ƒå‚æ•°é…ç½®å¯¹è±¡ï¼Œæ§åˆ¶ epochã€batch size ç­‰è¶…å‚æ•° |
| **train_dataset** | `Dataset` | âœ… | - | è®­ç»ƒæ•°æ®é›†ï¼Œéœ€å®ç° `__len__` å’Œ `__getitem__` æ–¹æ³• |
| **eval_dataset** | `Dataset` | âŒ | `None` | éªŒè¯æ•°æ®é›†ï¼ˆç»“æ„ä¸è®­ç»ƒé›†ä¸€è‡´ï¼‰ï¼Œä¸ºç©ºæ—¶ç¦ç”¨è¯„ä¼° |
| **compute_metrics** | `Callable` | âŒ | `None` | è‡ªå®šä¹‰è¯„ä¼°å‡½æ•°ï¼Œéœ€è¿”å›æŒ‡æ ‡å­—å…¸ |
| **label_names** | `List[str]` | âŒ | `None` | æ•°æ®é›†ä¸­æ ‡ç­¾å­—æ®µçš„åç§°ï¼ˆå¯¹åº” CSV ä¸­çš„åˆ—åï¼‰ |
| **callbacks** | `List[TrainerCallback]` | âŒ | `None` | è‡ªå®šä¹‰å›è°ƒåˆ—è¡¨ï¼ˆå¦‚æ—©åœã€æ—¥å¿—å¢å¼ºç­‰ï¼‰ |
| **optimizers** | `Tuple[Optimizer, LambdaLR]` | âŒ | `(None, None)` | è‡ªå®šä¹‰ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ |
| **tokenizer** | `PreTrainedTokenizer` | âŒ | `None` | åˆ†è¯å™¨å®ä¾‹ï¼ˆç”¨äºæ—¥å¿—è®°å½•è¾“å…¥æ ·æœ¬ï¼‰ |
| **data_collator** | `DataCollator` | âŒ | `default_data_collator` | æ‰¹æ•°æ®æ•´ç†å‡½æ•° |
- `gpt_classification_model.py`è®­ç»ƒå™¨ç›¸å…³ä»£ç 
```python
trainer = Trainer(
    model=model,                      # å·²åŠ è½½LoRAçš„æ¨¡å‹
    args=training_args,               # é…ç½®å¥½çš„è®­ç»ƒå‚æ•°
    train_dataset=train_dataset,      # å¹³è¡¡åçš„è®­ç»ƒé›†
    eval_dataset=val_dataset,         # éªŒè¯é›†
    label_names=["Label"],            # æ ‡ç­¾å­—æ®µå
    compute_metrics=lambda p: {       # è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
        "accuracy": accuracy_score(
            p.label_ids,              # çœŸå®æ ‡ç­¾
            p.predictions.argmax(-1)  # é¢„æµ‹ç±»åˆ«
        )
    },
)
```

### 7. è®­ç»ƒå›è°ƒæœºåˆ¶

**è®­ç»ƒå›è°ƒæœºåˆ¶è¯¦è§£**
- å®æ—¶ç›‘æ§æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„å‡†ç¡®ç‡å˜åŒ–ï¼Œç”¨äºæ£€æµ‹è¿‡æ‹Ÿåˆå’Œè®­ç»ƒç¨³å®šæ€§ã€‚
- ä¸å½±å“æ­£å¸¸è®­ç»ƒæµç¨‹ï¼Œä»…è¯»å–æ¨¡å‹å½“å‰çŠ¶æ€ã€‚
- æ—¥å¿—æ•°æ®è‡ªåŠ¨å…¼å®¹TensorBoardç­‰ç›‘æ§å·¥å…·ã€‚
- æ”¯æŒå¤šä¸ªå›è°ƒçš„é“¾å¼è°ƒç”¨ã€‚

**å›è°ƒæ³¨å†Œæœºåˆ¶æ‰§è¡Œæµç¨‹**

**1. åˆå§‹åŒ–ç»‘å®š**
- å›è°ƒå®ä¾‹ä¼šæŒæœ‰è®­ç»ƒå™¨(`trainer`)çš„å¼•ç”¨  
- å»ºç«‹åŒå‘é€šä¿¡é€šé“

**2. äº‹ä»¶æ³¨å†Œ**
- å°†å›è°ƒåŠ å…¥è®­ç»ƒå™¨çš„äº‹ä»¶ç›‘å¬é˜Ÿåˆ—  
- æ”¯æŒå¤šä¸ªå›è°ƒçš„é“¾å¼è°ƒç”¨

**3. è‡ªåŠ¨è§¦å‘**
| è®­ç»ƒé˜¶æ®µ       | å¯¹åº”å›è°ƒæ–¹æ³•     |
|---------------|------------------|
| è¯„ä¼°å¼€å§‹       | `on_evaluate()`  |
| æ£€æŸ¥ç‚¹ä¿å­˜     | `on_save()`      |
| è®­ç»ƒä¸­æ–­       | `on_interrupt()` |

**å›è°ƒæ³¨å†Œæœºåˆ¶åº”ç”¨åœºæ™¯**
- æ¨¡å‹æ€§èƒ½ç›‘æ§ï¼ˆå¦‚å‡†ç¡®ç‡/æŸå¤±æ›²çº¿ï¼‰
- è®­ç»ƒè¿‡ç¨‹æ§åˆ¶ï¼ˆæ—©åœ/å­¦ä¹ ç‡è°ƒæ•´ï¼‰  
- è‡ªå®šä¹‰æ—¥å¿—è®°å½•  
- èµ„æºç›‘æ§ï¼ˆæ˜¾å­˜/GPUåˆ©ç”¨ç‡ï¼‰
- `gpt_classification_model.py`å›è°ƒAccuracyCallbackç›¸å…³ä»£ç ï¼š
```python
class AccuracyCallback(TrainerCallback):
      def on_evaluate(self, args, state, control, kwargs):
          # å®æ—¶è®¡ç®—è®­ç»ƒé›†å‡†ç¡®ç‡
          train_pred = self.trainer.predict(self.trainer.train_dataset)
          train_acc = accuracy_score(train_pred.label_ids, 
                                    train_pred.predictions.argmax(-1))
          self.trainer.log({"train_accuracy": train_acc})
```
- æ ¸å¿ƒæ–¹æ³•ï¼š
  - on_evaluate()ï¼šåœ¨æ¯æ¬¡è¯„ä¼°é˜¶æ®µè‡ªåŠ¨è§¦å‘  
  - å†…éƒ¨ä½¿ç”¨è®­ç»ƒå™¨çš„predict()æ–¹æ³•è·å–å½“å‰æ¨¡å‹é¢„æµ‹ç»“æœ  
  - é€šè¿‡accuracy_scoreè®¡ç®—é¢„æµ‹å€¼ä¸çœŸå®æ ‡ç­¾çš„åŒ¹é…åº¦  
  - ç»“æœé€šè¿‡log()æ–¹æ³•è®°å½•åˆ°è®­ç»ƒæ—¥å¿—

### 8. è®­ç»ƒã€ä¿å­˜æ¨¡å‹ä¸è¯„ä¼°é¢„æµ‹

- `gpt_classification_model.py`ç›¸å…³ä»£ç ï¼š
```python
# å¼€å§‹è®­ç»ƒ
trainer.train()

# ä¿å­˜æœ€ä½³æ¨¡å‹(é€‚é…å™¨+åˆ†ç±»å¤´)
trainer.model.save_pretrained(
    "./lora_model",
    safe_serialization=True  # ä½¿ç”¨safetensorsæ ¼å¼
)
# ä¿å­˜tokenizerå’Œconfig
tokenizer.save_pretrained("./lora_model")
trainer.model.config.save_pretrained("./lora_model")

# éªŒè¯é›†è¯„ä¼°
eval_results = trainer.evaluate()
print(f"Validation results: {eval_results}")

# æµ‹è¯•é›†é¢„æµ‹
test_results = trainer.predict(test_dataset)
print(f"Test results: {test_results.metrics}")
```

**è®­ç»ƒè¿‡ç¨‹ä¸ç›¸å…³æŠ€æœ¯**
| é˜¶æ®µ         | å›¾æ ‡ | è¯´æ˜                     | æŠ€æœ¯ç»†èŠ‚                                                                 |
|--------------|------|--------------------------|--------------------------------------------------------------------------|
| **æ•°æ®åŠ è½½** | ğŸ“Š   | åŠ¨æ€æ‰¹å¤„ç†ä¸å¡«å……         | - è‡ªåŠ¨å°†æ•°æ®åˆ†æ‰¹æ¬¡<br>- åº”ç”¨paddingç»Ÿä¸€é•¿åº¦<br>- ç”Ÿæˆattention_mask       |
| **å‰å‘ä¼ æ’­** | â©   | è®¡ç®—æ¨¡å‹è¾“å‡º             | - è¾“å…¥é€šè¿‡æ‰€æœ‰ç½‘ç»œå±‚<br>- è¾“å‡ºåˆ†ç±»logits<br>- ä½¿ç”¨å½“å‰å‚æ•°è®¡ç®—            |
| **æŸå¤±è®¡ç®—** | ğŸ“‰   | äº¤å‰ç†µæŸå¤±å‡½æ•°           | - æ¯”è¾ƒé¢„æµ‹ä¸çœŸå®æ ‡ç­¾<br>- è®¡ç®—åˆ†ç±»è¯¯å·®<br>- å…¬å¼: L = -Î£y*log(p)          |
| **åå‘ä¼ æ’­** | ğŸ”™   | è‡ªåŠ¨å¾®åˆ†æ±‚æ¢¯åº¦           | - è®¡ç®—å„å‚æ•°æ¢¯åº¦<br>- é“¾å¼æ³•åˆ™é€å±‚ä¼ æ’­<br>- è®°å½•è®¡ç®—å›¾                    |
| **å‚æ•°æ›´æ–°** | ğŸ”„   | Adamä¼˜åŒ–å™¨è°ƒæ•´å‚æ•°       | - è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´<br>- åŠ¨é‡åŠ é€Ÿæ”¶æ•›                                     |
| **è¯„ä¼°æ£€æŸ¥ç‚¹** | âœ…   | éªŒè¯é›†æ€§èƒ½ç›‘æ§           | - è®¡ç®—éªŒè¯é›†æŒ‡æ ‡<br>- ä¿å­˜æœ€ä½³æ¨¡å‹<br>- æ—©åœæœºåˆ¶æ£€æµ‹                    |

**ä¿å­˜æ¨¡å‹**
| æ–‡ä»¶å | æ–‡ä»¶ç±»å‹ | å†…å®¹æè¿° | æ˜¯å¦å¿…éœ€ | å®‰å…¨ç‰¹æ€§ |
|--------|----------|----------|----------|----------|
| **adapter_config.json** | é…ç½®æ–‡ä»¶ | åŒ…å«LoRAé…ç½®å‚æ•°ï¼š<br>- ç§©(r)<br>- ç¼©æ”¾å› å­(alpha)<br>- ç›®æ ‡æ¨¡å—åˆ—è¡¨ | âœ… | JSON SchemaéªŒè¯ |
| **adapter_model.safetensors** | æƒé‡æ–‡ä»¶ | äºŒè¿›åˆ¶æ ¼å¼å­˜å‚¨ï¼š<br>- LoRAé€‚é…å™¨æƒé‡<br>- åˆ†ç±»å¤´å‚æ•° | âœ… | SafetensorsåŠ å¯† |
| **config.json** | é…ç½®æ–‡ä»¶ | åŸºç¡€æ¨¡å‹æ¶æ„ï¼š<br>- éšè—å±‚ç»´åº¦<br>- æ³¨æ„åŠ›å¤´æ•°<br>- pad_token_id | âœ… | ç‰ˆæœ¬ç­¾åæ ¡éªŒ |
| **tokenizer_config.json** | é…ç½®æ–‡ä»¶ | åˆ†è¯å¤„ç†è§„åˆ™ï¼š<br>- paddingç­–ç•¥<br>- truncationè®¾ç½®<br>- ç‰¹æ®Štokenå®šä¹‰ | âœ… | ç¼–ç è§„èŒƒæ£€æŸ¥ |
| **tokenizer.json** | åˆ†è¯å™¨æ–‡ä»¶ | æ–°æ ¼å¼æ•´åˆï¼š<br>- è¯æ±‡è¡¨<br>- åˆå¹¶è§„åˆ™<br>- åˆ†è¯ç®—æ³•é…ç½® | âœ… | å®Œæ•´æ€§å“ˆå¸Œ |
| **special_tokens_map.json** | æ˜ å°„æ–‡ä»¶ | ç‰¹æ®Štokenå¯¹åº”å…³ç³»ï¼š<br>- [PAD]<br>- [CLS]<br>- [SEP] | âŒ | UTF-8å¼ºåˆ¶ |
| **vocab.json** | è¯è¡¨æ–‡ä»¶ | æ—§æ ¼å¼è¯è¡¨ï¼š<br>- tokenåˆ°IDçš„æ˜ å°„ | âŒï¼ˆå…¼å®¹æ—§ç‰ˆéœ€è¦ï¼‰ | é”®å€¼æ ¡éªŒ |
| **merges.txt** | è§„åˆ™æ–‡ä»¶ | BPEåˆå¹¶è§„åˆ™ï¼š<br>- å­—ç¬¦çº§åˆå¹¶é¡ºåº | âŒï¼ˆBPEåˆ†è¯å™¨éœ€è¦ï¼‰ | è¡Œæ ¼å¼æ ¡éªŒ |

**éªŒè¯ä¸æµ‹è¯•**
- éªŒè¯é›†è¯„ä¼°æŒ‡æ ‡
  
| æŒ‡æ ‡åç§°                  | ç±»å‹    | è¯´æ˜                                                                 |
|---------------------------|---------|----------------------------------------------------------------------|
| `eval_loss`               | float   | éªŒè¯é›†ä¸Šçš„å¹³å‡äº¤å‰ç†µæŸå¤±å€¼                                           |
| `eval_accuracy`           | float   | åˆ†ç±»å‡†ç¡®ç‡ï¼ˆå½“é…ç½®compute_metricsæ—¶ï¼‰                               |
| `eval_runtime`            | float   | è¯„ä¼°æ€»è€—æ—¶ï¼ˆç§’ï¼‰                                                    |
| `eval_samples_per_second` | float   | æ¯ç§’å¤„ç†çš„æ ·æœ¬æ•°ï¼ˆååé‡ï¼‰                                           |
| `eval_steps_per_second`   | float   | æ¯ç§’å®Œæˆçš„æ‰¹æ¬¡æ•°                                                    |

- æµ‹è¯•é›†é¢„æµ‹è¾“å‡º

| å±æ€§/æŒ‡æ ‡              | ç±»å‹       | è¯´æ˜                                                                 |
|------------------------|------------|----------------------------------------------------------------------|
| `predictions`          | np.ndarray | åŸå§‹é¢„æµ‹logitsï¼ˆå½¢çŠ¶ï¼š[æ ·æœ¬æ•°Ã—ç±»åˆ«æ•°]ï¼‰                              |
| `label_ids`            | np.ndarray | çœŸå®æ ‡ç­¾ï¼ˆå½¢çŠ¶ï¼š[æ ·æœ¬æ•°]ï¼‰                                           |
| `metrics`              | dict       | åŒ…å«ä»¥ä¸‹æµ‹è¯•æŒ‡æ ‡ï¼š                                                  |
| â†³ `test_loss`          | float      | æµ‹è¯•é›†ä¸Šçš„å¹³å‡æŸå¤±å€¼                                                |
| â†³ `test_accuracy`      | float      | åˆ†ç±»å‡†ç¡®ç‡ï¼ˆå½“é…ç½®compute_metricsæ—¶ï¼‰                               |
| â†³ `test_samples`       | int        | æµ‹è¯•æ ·æœ¬æ€»æ•° 

### 9. è®­ç»ƒæŒ‡æ ‡å¯è§†åŒ–
- `gpt_classification_model.py`ç›¸å…³ä»£ç ï¼š
```python
# ç”Ÿæˆè®­ç»ƒæŒ‡æ ‡å›¾è¡¨
plot_metrics(trainer.state.log_history)
```

**è¾“å…¥è¾“å‡º**
- è¾“å…¥ï¼š`trainer.state.log_history`ï¼ˆè®­ç»ƒæ—¥å¿—å­—å…¸åˆ—è¡¨ï¼‰
- è¾“å‡ºï¼šç”ŸæˆPDFæ ¼å¼çš„åŒæ›²çº¿å¯¹æ¯”å›¾ï¼š
  - è®­ç»ƒé›† vs éªŒè¯é›†å‡†ç¡®ç‡æ›²çº¿
  - è®­ç»ƒé›† vs éªŒè¯é›†æŸå¤±å€¼æ›²çº¿

**å›¾è¡¨è¦ç´ è¯´æ˜**

- å‡†ç¡®ç‡å¯¹æ¯”æ›²çº¿

| å…ƒç´         | è¯´æ˜                          |
|-------------|-------------------------------|
| **Xè½´**     | è®­ç»ƒæ­¥æ•°ï¼ˆTraining Stepsï¼‰     |
| **Yè½´**     | åˆ†ç±»å‡†ç¡®ç‡ï¼ˆ0-1èŒƒå›´ï¼‰          |
| **è®­ç»ƒæ›²çº¿** | è“è‰²å®çº¿ï¼ˆTrain Accuracyï¼‰     |
| **éªŒè¯æ›²çº¿** | æ©™è‰²è™šçº¿ï¼ˆVal Accuracyï¼‰       |
| **æ ¸å¿ƒåŠŸèƒ½** | ç›‘æ§æ¨¡å‹æ”¶æ•›/è¿‡æ‹Ÿåˆè¶‹åŠ¿        |

- æŸå¤±å€¼å¯¹æ¯”æ›²çº¿

| å…ƒç´         | è¯´æ˜                          |
|-------------|-------------------------------|
| **Xè½´**     | è®­ç»ƒæ­¥æ•°ï¼ˆTraining Stepsï¼‰     |
| **Yè½´**     | äº¤å‰ç†µæŸå¤±å€¼ï¼ˆè¶Šå°è¶Šå¥½ï¼‰       |
| **è®­ç»ƒæ›²çº¿** | è“è‰²å®çº¿ï¼ˆTrain Lossï¼‰        |
| **éªŒè¯æ›²çº¿** | æ©™è‰²è™šçº¿ï¼ˆVal Lossï¼‰          |
| **æ ¸å¿ƒåŠŸèƒ½** | è§‚å¯Ÿä¼˜åŒ–è¿‡ç¨‹å’Œå­¦ä¹ ç‡æ•ˆæœ       |

## chainlitå¯è§†åŒ–äº¤äº’ç•Œé¢

### 1. æ¨¡å‹åŠ è½½æ¨¡å—
- æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨lora_modelç›®å½•
- åŠ è½½åŸºç¡€GPT-2åˆ†è¯å™¨å¹¶é…ç½®pad_token
- åˆå§‹åŒ–åŸºç¡€GPT-2åˆ†ç±»æ¨¡å‹(äºŒåˆ†ç±»)
- åŠ è½½LoRAé€‚é…å™¨å¹¶åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹
- å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼å¹¶è¿”å›ç»„ä»¶
- `chainlit_gpt_classification.py`ç›¸å…³ä»£ç ï¼š
```python
def get_model_and_tokenizer():
    """
    åŠ è½½æœ¬åœ°ä¿å­˜çš„LoRAå¾®è°ƒæ¨¡å‹å’Œåˆ†è¯å™¨
    å¦‚æœæ¨¡å‹ç›®å½•ä¸å­˜åœ¨, åˆ™è¾“å‡ºé”™è¯¯å¹¶é€€å‡ºç¨‹åº
    """
    model_path = './lora_model'  # LoRAé€‚é…å™¨

    if not os.path.exists(model_path):
        print(f"æ‰¾ä¸åˆ° {model_path} ç›®å½•, è¯·ç¡®è®¤æ¨¡å‹å·²æ­£ç¡®ä¿å­˜")
        sys.exit(1)

    # åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token  # ä½¿ç”¨eos_tokenä½œä¸ºpad_token

    base_model = AutoModelForSequenceClassification.from_pretrained(
        "gpt2",
        num_labels=2,
        pad_token_id=tokenizer.eos_token_id
    )

    # åŠ è½½LoRAé€‚é…å™¨
    model = PeftModel.from_pretrained(base_model, model_path)

    # åˆå¹¶é€‚é…å™¨åˆ°åŸºç¡€æ¨¡å‹
    model = model.merge_and_unload()
    model.to(device)
    model.eval()

    return tokenizer, model
```

### 2. æ–‡æœ¬åˆ†ç±»æ¨¡å—
- æ¥æ”¶ç”¨æˆ·è¾“å…¥æ–‡æœ¬
- ä½¿ç”¨åˆ†è¯å™¨è¿›è¡Œç¼–ç å’Œå¡«å……(max_length=120)
- å°†è¾“å…¥è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶é€å…¥device(CPU/GPU)
- æ‰§è¡Œæ¨¡å‹æ¨ç†(ç¦ç”¨æ¢¯åº¦è®¡ç®—)
- è§£ælogitsè¾“å‡ºå¾—åˆ°é¢„æµ‹ç±»åˆ«
- è¿”å›"åƒåœ¾ä¿¡æ¯"æˆ–"æ­£å¸¸ä¿¡æ¯"åˆ†ç±»ç»“æœ
- `chainlit_gpt_classification.py`ç›¸å…³ä»£ç ï¼š
```python
def classify_review(user_input):
    """
    å¯¹ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œåƒåœ¾çŸ­ä¿¡åˆ†ç±»(spam/ham)
    """
    tokenizer, model = get_model_and_tokenizer()

    inputs = tokenizer(
        user_input,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=120
    ).to(device)

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        predicted_class = torch.argmax(logits, dim=-1).item()

    return "åƒåœ¾ä¿¡æ¯" if predicted_class == 1 else "æ­£å¸¸ä¿¡æ¯"
```

### 3. Chainlitäº¤äº’æ¨¡å—
- å¼‚æ­¥ç›‘å¬ç”¨æˆ·æ¶ˆæ¯è¾“å…¥
- éªŒè¯è¾“å…¥éç©º
- è°ƒç”¨åˆ†ç±»å‡½æ•°è·å–ç»“æœ
- é€šè¿‡Chainlitè¿”å›æ ¼å¼åŒ–ç»“æœ
- æ•è·å¹¶æ˜¾ç¤ºå¼‚å¸¸ä¿¡æ¯
- `chainlit_gpt_classification.py`ç›¸å…³ä»£ç ï¼š
```python
@chainlit.on_message
async def main(message: chainlit.Message):
    """
    Chainlit çš„ä¸»æ¶ˆæ¯å¤„ç†å‡½æ•°
    æ¥æ”¶ç”¨æˆ·è¾“å…¥, å¹¶è¿”å›åˆ†ç±»ç»“æœ
    """
    user_input = message.content

    if not user_input:
        await chainlit.Message(content="è¯·è¾“å…¥ä¸€æ®µæ–‡æœ¬ä»¥è¿›è¡Œåˆ†ç±»").send()
        return

    try:
        label = classify_review(user_input)
        await chainlit.Message(content=f"è¯¥æ–‡æœ¬è¢«åˆ†ç±»ä¸º: {label}").send()
    except Exception as e:
        await chainlit.Message(content=f"é”™è¯¯ä¿¡æ¯: {str(e)}").send()
```
